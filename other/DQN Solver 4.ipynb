{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda8b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd080e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2dff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0e191",
   "metadata": {},
   "source": [
    "Inspiration: https://andrew-gordienko.medium.com/reinforcement-learning-dqn-w-pytorch-7c6faad3d1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea775c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 2_000\n",
    "LEARNING_RATE = 0.001\n",
    "MEM_SIZE = 1000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9999\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.0025\n",
    "EXPLORATION_DECAY = 0.9999 # math.exp(math.log(EXPLORATION_MIN)/EPISODES) \n",
    "EXPLORATION_DECAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4462554",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a915b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply a collection of methods to work with the state / observed environment\n",
    "class NQEnv:\n",
    "    def __init__(self, n, k):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.rows, self.cols, self.diags, self.anti_diags = set(), set(), set(), set()\n",
    "        self.board = np.zeros(self.n * self.n)\n",
    "        \n",
    "    def reset(self) -> np.array:\n",
    "        \"\"\"\n",
    "            Resets board with new partial N-Queens configuration\n",
    "        \"\"\"        \n",
    "        has_set = False\n",
    "        while not has_set:\n",
    "            self._reset() # resets board and line sets\n",
    "            num_set = 0\n",
    "            while num_set < self.k:\n",
    "                if not self.get_avail_squares(): break\n",
    "                action = self.sample_action()\n",
    "                self._perform_action(action)\n",
    "                \n",
    "                num_set += 1\n",
    "                \n",
    "            has_set = num_set == self.k\n",
    "    \n",
    "        return self.board # 1D array -> partial config\n",
    "    \n",
    "    def _perform_action(self, action:int):\n",
    "        \"\"\"\n",
    "            Performs action i.e. places queen on square (action//n, action (mod n))\n",
    "        \"\"\"\n",
    "        row,col = action//self.n, action%self.n\n",
    "        self.rows.add(row)\n",
    "        self.cols.add(col)\n",
    "        self.diags.add(col-row)\n",
    "        self.anti_diags.add(col+row)\n",
    "        \n",
    "        self.board[action] = 1\n",
    "        \n",
    "    def get_avail_squares(self) -> List[int]:\n",
    "        \"\"\"\n",
    "            Returns a list of the squares available in col-major format\n",
    "        \"\"\"\n",
    "        \n",
    "        avail = []\n",
    "        for row, col in itertools.product(range(self.n), range(self.n)):\n",
    "            if row in self.rows or col in self.cols or col-row in self.diags or col+row in self.anti_diags: continue\n",
    "            avail.append(self.n*row + col)\n",
    "        return avail\n",
    "    \n",
    "    def sample_action(self):\n",
    "        \"\"\"\n",
    "            Returns a valid action, assumes such an action exists\n",
    "        \"\"\"\n",
    "        avail = self.get_avail_squares()\n",
    "        return random.sample(avail,1)[0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "            Performs action on the board, assumes that the action is valid\n",
    "        \"\"\"\n",
    "        self._perform_action(action)\n",
    "        avail = self.get_avail_squares()\n",
    "        terminate = len(self.rows) == self.n or len(avail) == 0\n",
    "        reward = (len(self.rows)/self.n)**4 / (1 if len(self.rows) == self.n else 5)\n",
    "        return (self.board.copy(), reward, terminate) # new_state, reward, teminated\n",
    "    \n",
    "    def get_mask(self):\n",
    "        avail = self.get_avail_squares()\n",
    "        mask = [0] * self.n * self.n\n",
    "        for idx in avail: mask[idx] = 1\n",
    "        return torch.as_tensor(mask)\n",
    "    \n",
    "    def render(self, fontsize=20):\n",
    "        \"\"\"\n",
    "            Render the board nicely\n",
    "        \"\"\"\n",
    "        matrix = np.zeros((self.n, self.n), dtype=int)\n",
    "        for i in range(self.n**2):\n",
    "            matrix[i//self.n][i%self.n] = int(self.board[i])\n",
    "        \n",
    "        n = matrix.shape[0]\n",
    "        # Create a chess board (n x n) pattern\n",
    "        board = np.zeros_like(matrix)\n",
    "        board[1::2, ::2] = 1\n",
    "        board[::2, 1::2] = 1\n",
    "\n",
    "        cmap = ListedColormap(['#769656', '#eeeed2'])\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(board, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "        # Place queens based on matrix\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if matrix[i, j] == 1:\n",
    "                    ax.text(j, i, 'â™›', fontsize=fontsize, ha='center', va='center', color='black' if board[i, j] else 'white')\n",
    "\n",
    "        # Hide the axes\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269a6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NQEnv at 0x1dd89323eb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = NQEnv(N,k)\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada0114",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd21ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        _observation_size = N*N\n",
    "        _action_space_size = N*N\n",
    "        self.fc1 = nn.Linear(_observation_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc5 = nn.Linear(512, 256)\n",
    "        self.fc6 = nn.Linear(256, 128)\n",
    "        self.out = nn.Linear(128, _action_space_size)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d28180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.mem_count = 0\n",
    "        \n",
    "        _observationspace_shape = N*N\n",
    "        self.states = np.zeros((MEM_SIZE, _observationspace_shape),dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, _observationspace_shape),dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
    "        \n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        mem_index = self.mem_count % MEM_SIZE\n",
    "\n",
    "        self.states[mem_index]  = state\n",
    "        self.actions[mem_index] = action\n",
    "        self.rewards[mem_index] = reward\n",
    "        self.states_[mem_index] = state_\n",
    "        self.dones[mem_index] =  1 - done\n",
    "\n",
    "        self.mem_count += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "        \n",
    "        states  = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        dones   = self.dones[batch_indices]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69203a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a7904",
   "metadata": {},
   "source": [
    "### Training framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56504155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Solver:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = DQN()\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.sample_action()\n",
    "        \n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.to(device)\n",
    "        state = state.unsqueeze(0)\n",
    "        q_values = self.network(state)\n",
    "        \n",
    "        mask = env.get_mask()\n",
    "        q_values *= mask\n",
    "        \n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_count < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states , dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        q_values = self.network(states)\n",
    "        next_q_values = self.network(states_)\n",
    "        \n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "        \n",
    "        q_target = rewards + GAMMA * predicted_value_of_future * dones\n",
    "\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087bfc8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427799fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = N*N\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc49d6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\AppData\\Local\\Temp\\ipykernel_17044\\1939111559.py:10: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e67a74eb8ad48498ae3c5a69f13aabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Initializing:   0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m state_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state_, [\u001b[38;5;241m1\u001b[39m, observation_space])\n\u001b[0;32m     15\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39madd(state, action, reward, state_, done)\n\u001b[1;32m---> 16\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m state \u001b[38;5;241m=\u001b[39m state_\n\u001b[0;32m     18\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn [9], line 44\u001b[0m, in \u001b[0;36mDQN_Solver.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m EXPLORATION_DECAY\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(EXPLORATION_MIN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQN_Solver()\n",
    "\n",
    "# Create a tqdm progress bar for episodes\n",
    "pbar = tqdm(range(1, EPISODES), desc='Initializing')\n",
    "\n",
    "for i in pbar:\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            # Update progress bar description with the latest episode info instead of using print\n",
    "            pbar.set_description(\"Episode: {} Avg Reward: {:.2f} Best Reward: {} Last Reward: {} Epsilon: {:.2f}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            pbar.refresh() # to show immediately the update\n",
    "            break\n",
    "            \n",
    "    episode_number.append(i)\n",
    "    average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Reward\")\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be660efb",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_history(state_history, n):\n",
    "    # Adjusting figsize and subplot layout for vertical display\n",
    "    fig, axs = plt.subplots(len(state_history), 1, figsize=(4, len(state_history)*4))\n",
    "    \n",
    "    # Ensuring axs is iterable when there's only one plot\n",
    "    if len(state_history) == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for ax, state in zip(axs, state_history):\n",
    "        matrix = state.reshape((n, n))  # Assuming state is a flat array\n",
    "        board = np.zeros_like(matrix)\n",
    "        board[1::2, ::2] = 1\n",
    "        board[::2, 1::2] = 1\n",
    "        cmap = ListedColormap(['#769656', '#eeeed2'])\n",
    "        ax.imshow(board, cmap=cmap, interpolation='nearest')\n",
    "        \n",
    "        # Adding the queens to the board\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if matrix[i, j] == 1:\n",
    "                    ax.text(j, i, 'â™›', fontsize=10, ha='center', va='center', color='black' if board[i, j] else 'white')\n",
    "        \n",
    "        # Removing the ticks from the axes\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622caeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_history = []\n",
    "action_history = []\n",
    "avail_actions_history = []\n",
    "\n",
    "# Initial random partial config\n",
    "state = env.reset()\n",
    "state_history.append(state.copy())\n",
    "\n",
    "# Place queens\n",
    "while len(env.get_avail_squares()):\n",
    "    avail_actions_history.append(env.get_avail_squares())\n",
    "    \n",
    "    action = agent.choose_action(state)\n",
    "    env._perform_action(action)\n",
    "    state = env.board\n",
    "    state_history.append(state.copy())\n",
    "    action_history.append(action)\n",
    "    \n",
    "print(f\"Number of queens placed = {k + len(state_history) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b27a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.render()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489eb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_history, avail_actions_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eed8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_state_history(state_history, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da6237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24467a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590ab60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e12953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcac21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086005d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac320ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04627c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
